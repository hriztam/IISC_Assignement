{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839f30be",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 1: Install Dependencies\n",
    "# ============================================\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q opencv-python pillow matplotlib numpy\n",
    "!pip install -q supervision transformers\n",
    "\n",
    "# Clone and install SAM 2\n",
    "!git clone https://github.com/facebookresearch/segment-anything-2.git\n",
    "%cd segment-anything-2\n",
    "!pip install -q -e .\n",
    "%cd ..\n",
    "\n",
    "# Clone and install GroundingDINO\n",
    "!git clone https://github.com/IDEA-Research/GroundingDINO.git\n",
    "%cd GroundingDINO\n",
    "!pip install -q -e .\n",
    "%cd ..\n",
    "\n",
    "# Download SAM 2 checkpoints\n",
    "!mkdir -p checkpoints\n",
    "!wget -q https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt -P checkpoints/\n",
    "!wget -q https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_base_plus.pt -P checkpoints/\n",
    "\n",
    "# Download GroundingDINO weights\n",
    "!mkdir -p weights\n",
    "!wget -q https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth -P weights/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fe8487",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 2: Import Libraries and Setup\n",
    "# ============================================\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add paths\n",
    "sys.path.append('./segment-anything-2')\n",
    "sys.path.append('./GroundingDINO')\n",
    "\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "from sam2.build_sam import build_sam2_video_predictor\n",
    "\n",
    "# Import GroundingDINO\n",
    "from groundingdino.util.inference import load_model, load_image, predict, annotate\n",
    "import groundingdino.datasets.transforms as T\n",
    "from torchvision.ops import box_convert\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b935706",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    " ============================================\n",
    "# CELL 3: Helper Functions\n",
    "# ============================================\n",
    "def download_sample_image():\n",
    "    \"\"\"Download a sample image for testing\"\"\"\n",
    "    !wget -q https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/dog.jpg -O sample_image.jpg\n",
    "    return \"sample_image.jpg\"\n",
    "\n",
    "def download_sample_video():\n",
    "    \"\"\"Download a sample video for testing\"\"\"\n",
    "    # Download a short sample video\n",
    "    !wget -q https://test-videos.co.uk/vids/bigbuckbunny/mp4/h264/360/Big_Buck_Bunny_360_10s_1MB.mp4 -O sample_video.mp4\n",
    "    return \"sample_video.mp4\"\n",
    "\n",
    "def show_mask(mask, ax, color=[30/255, 144/255, 255/255, 0.6]):\n",
    "    \"\"\"Display mask on axis\"\"\"\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * np.array(color).reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def show_box(box, ax, label):\n",
    "    \"\"\"Display bounding box on axis\"\"\"\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', \n",
    "                               facecolor=(0,0,0,0), lw=2))\n",
    "    ax.text(x0, y0-5, label, fontsize=12, color='white', \n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='green', alpha=0.7))\n",
    "\n",
    "def process_grounding_dino_output(boxes, logits, phrases, image_shape):\n",
    "    \"\"\"Convert GroundingDINO output to SAM format\"\"\"\n",
    "    h, w = image_shape[:2]\n",
    "    boxes = boxes * torch.tensor([w, h, w, h])\n",
    "    xyxy = box_convert(boxes=boxes, in_fmt=\"cxcywh\", out_fmt=\"xyxy\").numpy()\n",
    "    return xyxy, logits.numpy(), phrases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070ca917",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 4: Initialize Models\n",
    "# ============================================\n",
    "class TextPromptedSegmentor:\n",
    "    def __init__(self, sam_checkpoint=\"checkpoints/sam2_hiera_large.pt\",\n",
    "                 grounding_dino_checkpoint=\"weights/groundingdino_swint_ogc.pth\"):\n",
    "        \n",
    "        # Initialize SAM 2\n",
    "        self.sam_model = build_sam2(\n",
    "            config_file=\"segment-anything-2/sam2_configs/sam2_hiera_l.yaml\",\n",
    "            ckpt_path=sam_checkpoint,\n",
    "            device=device\n",
    "        )\n",
    "        self.sam_predictor = SAM2ImagePredictor(self.sam_model)\n",
    "        \n",
    "        # Initialize GroundingDINO\n",
    "        self.grounding_model = load_model(\n",
    "            \"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\",\n",
    "            grounding_dino_checkpoint,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        print(\"✓ Models loaded successfully!\")\n",
    "    \n",
    "    def detect_objects(self, image_path, text_prompt, box_threshold=0.35, text_threshold=0.25):\n",
    "        \"\"\"Detect objects using GroundingDINO based on text prompt\"\"\"\n",
    "        image_source, image = load_image(image_path)\n",
    "        \n",
    "        # Predict with GroundingDINO\n",
    "        boxes, logits, phrases = predict(\n",
    "            model=self.grounding_model,\n",
    "            image=image,\n",
    "            caption=text_prompt,\n",
    "            box_threshold=box_threshold,\n",
    "            text_threshold=text_threshold,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # Convert to pixel coordinates\n",
    "        h, w, _ = image_source.shape\n",
    "        boxes_xyxy, scores, labels = process_grounding_dino_output(\n",
    "            boxes, logits, phrases, (h, w)\n",
    "        )\n",
    "        \n",
    "        return image_source, boxes_xyxy, scores, labels\n",
    "    \n",
    "    def segment_image(self, image_path, text_prompt, box_threshold=0.35):\n",
    "        \"\"\"Complete pipeline: text prompt → detection → segmentation\"\"\"\n",
    "        \n",
    "        # Step 1: Detect objects with GroundingDINO\n",
    "        image, boxes, scores, labels = self.detect_objects(\n",
    "            image_path, text_prompt, box_threshold\n",
    "        )\n",
    "        \n",
    "        if len(boxes) == 0:\n",
    "            print(f\"No objects detected for prompt: '{text_prompt}'\")\n",
    "            return image, None, None, None\n",
    "        \n",
    "        print(f\"✓ Detected {len(boxes)} object(s): {labels}\")\n",
    "        \n",
    "        # Step 2: Set image for SAM\n",
    "        self.sam_predictor.set_image(image)\n",
    "        \n",
    "        # Step 3: Generate masks for each detected box\n",
    "        masks = []\n",
    "        for box in boxes:\n",
    "            mask, score, _ = self.sam_predictor.predict(\n",
    "                box=box,\n",
    "                multimask_output=False\n",
    "            )\n",
    "            masks.append(mask[0])\n",
    "        \n",
    "        return image, boxes, labels, masks\n",
    "    \n",
    "    def visualize_results(self, image, boxes, labels, masks, text_prompt):\n",
    "        \"\"\"Visualize detection and segmentation results\"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 8))\n",
    "        \n",
    "        # Original image with boxes\n",
    "        axes[0].imshow(image)\n",
    "        axes[0].set_title(f\"Detection: '{text_prompt}'\")\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        if boxes is not None:\n",
    "            for box, label in zip(boxes, labels):\n",
    "                show_box(box, axes[0], label)\n",
    "        \n",
    "        # Segmentation masks\n",
    "        axes[1].imshow(image)\n",
    "        axes[1].set_title(f\"Segmentation: '{text_prompt}'\")\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        if masks is not None:\n",
    "            for mask, label in zip(masks, labels):\n",
    "                show_mask(mask, axes[1])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Initialize the segmentor\n",
    "segmentor = TextPromptedSegmentor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb579238",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    " ============================================\n",
    "# CELL 5: Single Image Segmentation Demo\n",
    "# ============================================\n",
    "# Download sample image\n",
    "image_path = download_sample_image()\n",
    "\n",
    "# Test different text prompts\n",
    "test_prompts = [\n",
    "    \"dog\",\n",
    "    \"dog face\",\n",
    "    \"animal\",\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\n📝 Processing prompt: '{prompt}'\")\n",
    "    image, boxes, labels, masks = segmentor.segment_image(\n",
    "        image_path, \n",
    "        prompt,\n",
    "        box_threshold=0.3\n",
    "    )\n",
    "    segmentor.visualize_results(image, boxes, labels, masks, prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d3846a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 6: Upload Custom Image (Optional)\n",
    "# ============================================\n",
    "from google.colab import files\n",
    "import io\n",
    "\n",
    "def upload_and_segment():\n",
    "    \"\"\"Upload your own image and segment with text prompt\"\"\"\n",
    "    print(\"Upload an image:\")\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    for filename in uploaded.keys():\n",
    "        # Save uploaded file\n",
    "        image_data = uploaded[filename]\n",
    "        image = Image.open(io.BytesIO(image_data))\n",
    "        image.save('uploaded_image.jpg')\n",
    "        \n",
    "        # Get text prompt\n",
    "        text_prompt = input(\"Enter text prompt for segmentation: \")\n",
    "        \n",
    "        # Process\n",
    "        image, boxes, labels, masks = segmentor.segment_image(\n",
    "            'uploaded_image.jpg',\n",
    "            text_prompt,\n",
    "            box_threshold=0.25\n",
    "        )\n",
    "        \n",
    "        # Visualize\n",
    "        segmentor.visualize_results(image, boxes, labels, masks, text_prompt)\n",
    "        \n",
    "        # Save results\n",
    "        if masks is not None:\n",
    "            result = image.copy()\n",
    "            for mask in masks:\n",
    "                colored_mask = np.zeros_like(image)\n",
    "                colored_mask[:, :, 0] = mask * 255\n",
    "                result = cv2.addWeighted(result, 0.7, colored_mask, 0.3, 0)\n",
    "            \n",
    "            cv2.imwrite('segmentation_result.jpg', cv2.cvtColor(result, cv2.COLOR_RGB2BGR))\n",
    "            print(\"✓ Result saved as 'segmentation_result.jpg'\")\n",
    "\n",
    "# Uncomment to use:\n",
    "# upload_and_segment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbfe5eb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 7: Video Object Segmentation\n",
    "# ============================================\n",
    "class VideoSegmentor:\n",
    "    def __init__(self, sam_checkpoint=\"checkpoints/sam2_hiera_large.pt\"):\n",
    "        # Initialize video predictor\n",
    "        self.video_predictor = build_sam2_video_predictor(\n",
    "            config_file=\"segment-anything-2/sam2_configs/sam2_hiera_l.yaml\",\n",
    "            ckpt_path=sam_checkpoint,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # Reuse GroundingDINO from image segmentor\n",
    "        self.grounding_model = segmentor.grounding_model\n",
    "        print(\"✓ Video segmentor ready!\")\n",
    "    \n",
    "    def extract_frames(self, video_path, max_frames=30):\n",
    "        \"\"\"Extract frames from video\"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        frame_count = 0\n",
    "        \n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        # Sample frames evenly\n",
    "        sample_rate = max(1, total_frames // max_frames)\n",
    "        \n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            if frame_count % sample_rate == 0:\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frames.append(frame_rgb)\n",
    "            \n",
    "            frame_count += 1\n",
    "            \n",
    "            if len(frames) >= max_frames:\n",
    "                break\n",
    "        \n",
    "        cap.release()\n",
    "        print(f\"✓ Extracted {len(frames)} frames\")\n",
    "        return frames, fps\n",
    "    \n",
    "    def detect_in_first_frame(self, frame, text_prompt, box_threshold=0.3):\n",
    "        \"\"\"Detect object in first frame using GroundingDINO\"\"\"\n",
    "        # Save frame temporarily\n",
    "        cv2.imwrite('temp_frame.jpg', cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "        \n",
    "        # Detect with GroundingDINO\n",
    "        _, boxes, scores, labels = segmentor.detect_objects(\n",
    "            'temp_frame.jpg',\n",
    "            text_prompt,\n",
    "            box_threshold\n",
    "        )\n",
    "        \n",
    "        return boxes, labels\n",
    "    \n",
    "    def segment_video(self, video_path, text_prompt, max_frames=30):\n",
    "        \"\"\"Complete video segmentation pipeline\"\"\"\n",
    "        \n",
    "        # Extract frames\n",
    "        frames, fps = self.extract_frames(video_path, max_frames)\n",
    "        \n",
    "        if len(frames) == 0:\n",
    "            print(\"Failed to extract frames\")\n",
    "            return None\n",
    "        \n",
    "        # Detect in first frame\n",
    "        boxes, labels = self.detect_in_first_frame(frames[0], text_prompt)\n",
    "        \n",
    "        if len(boxes) == 0:\n",
    "            print(f\"No objects detected for: '{text_prompt}'\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"✓ Found {len(boxes)} object(s): {labels}\")\n",
    "        \n",
    "        # Initialize video predictor with frames\n",
    "        inference_state = self.video_predictor.init_state(video_path=video_path)\n",
    "        \n",
    "        # Add detected objects as prompts\n",
    "        for idx, box in enumerate(boxes):\n",
    "            _, out_obj_ids, out_mask_logits = self.video_predictor.add_new_points_or_box(\n",
    "                inference_state=inference_state,\n",
    "                frame_idx=0,\n",
    "                obj_id=idx,\n",
    "                box=box\n",
    "            )\n",
    "        \n",
    "        # Propagate through video\n",
    "        video_segments = {}\n",
    "        for out_frame_idx, out_obj_ids, out_mask_logits in self.video_predictor.propagate_in_video(inference_state):\n",
    "            video_segments[out_frame_idx] = {\n",
    "                out_obj_id: out_mask_logits[i].cpu().numpy()\n",
    "                for i, out_obj_id in enumerate(out_obj_ids)\n",
    "            }\n",
    "        \n",
    "        return frames, video_segments, labels\n",
    "    \n",
    "    def visualize_video_results(self, frames, segments, labels, text_prompt, sample_frames=5):\n",
    "        \"\"\"Visualize video segmentation results\"\"\"\n",
    "        \n",
    "        # Sample frames to display\n",
    "        total_frames = len(frames)\n",
    "        indices = np.linspace(0, total_frames-1, sample_frames, dtype=int)\n",
    "        \n",
    "        fig, axes = plt.subplots(2, sample_frames, figsize=(20, 8))\n",
    "        \n",
    "        for col, idx in enumerate(indices):\n",
    "            # Original frame\n",
    "            axes[0, col].imshow(frames[idx])\n",
    "            axes[0, col].set_title(f\"Frame {idx}\")\n",
    "            axes[0, col].axis('off')\n",
    "            \n",
    "            # Segmented frame\n",
    "            axes[1, col].imshow(frames[idx])\n",
    "            \n",
    "            # Add masks if available\n",
    "            if idx in segments:\n",
    "                for obj_id, mask in segments[idx].items():\n",
    "                    mask_binary = (mask > 0.0).squeeze()\n",
    "                    show_mask(mask_binary, axes[1, col])\n",
    "            \n",
    "            axes[1, col].set_title(f\"Segmented\")\n",
    "            axes[1, col].axis('off')\n",
    "        \n",
    "        fig.suptitle(f\"Video Segmentation: '{text_prompt}'\", fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def save_video_with_masks(self, frames, segments, output_path='output_video.mp4', fps=30):\n",
    "        \"\"\"Save video with mask overlays\"\"\"\n",
    "        if not frames:\n",
    "            return\n",
    "        \n",
    "        h, w = frames[0].shape[:2]\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (w, h))\n",
    "        \n",
    "        for idx, frame in enumerate(frames):\n",
    "            result = frame.copy()\n",
    "            \n",
    "            # Add masks if available\n",
    "            if idx in segments:\n",
    "                for obj_id, mask in segments[idx].items():\n",
    "                    mask_binary = (mask > 0.0).squeeze().astype(np.uint8) * 255\n",
    "                    mask_colored = np.zeros_like(frame)\n",
    "                    mask_colored[:, :, 0] = mask_binary  # Red channel\n",
    "                    result = cv2.addWeighted(result, 0.7, mask_colored, 0.3, 0)\n",
    "            \n",
    "            out.write(cv2.cvtColor(result, cv2.COLOR_RGB2BGR))\n",
    "        \n",
    "        out.release()\n",
    "        print(f\"✓ Video saved as '{output_path}'\")\n",
    "\n",
    "# Initialize video segmentor\n",
    "video_seg = VideoSegmentor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf31053",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 8: Video Segmentation Demo\n",
    "# ============================================\n",
    "# Download sample video\n",
    "video_path = download_sample_video()\n",
    "\n",
    "# Segment video with text prompt\n",
    "text_prompt = \"rabbit\"\n",
    "print(f\"\\n🎥 Processing video with prompt: '{text_prompt}'\")\n",
    "\n",
    "frames, segments, labels = video_seg.segment_video(\n",
    "    video_path,\n",
    "    text_prompt,\n",
    "    max_frames=20  # Limit frames for Colab memory\n",
    ")\n",
    "\n",
    "if frames is not None:\n",
    "    # Visualize results\n",
    "    video_seg.visualize_video_results(\n",
    "        frames, segments, labels, text_prompt,\n",
    "        sample_frames=5\n",
    "    )\n",
    "    \n",
    "    # Save output video\n",
    "    video_seg.save_video_with_masks(\n",
    "        frames, segments,\n",
    "        output_path='segmented_video.mp4',\n",
    "        fps=15\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781710e4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    " ============================================\n",
    "# CELL 9: Interactive Demo Functions\n",
    "# ============================================\n",
    "def interactive_image_segmentation():\n",
    "    \"\"\"Interactive function for custom prompts\"\"\"\n",
    "    print(\"=== Interactive Image Segmentation ===\")\n",
    "    \n",
    "    # Option to upload or use sample\n",
    "    use_sample = input(\"Use sample image? (y/n): \").lower() == 'y'\n",
    "    \n",
    "    if use_sample:\n",
    "        image_path = download_sample_image()\n",
    "    else:\n",
    "        print(\"Upload your image:\")\n",
    "        uploaded = files.upload()\n",
    "        image_path = list(uploaded.keys())[0]\n",
    "    \n",
    "    while True:\n",
    "        text_prompt = input(\"\\nEnter text prompt (or 'quit' to exit): \")\n",
    "        if text_prompt.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        image, boxes, labels, masks = segmentor.segment_image(\n",
    "            image_path,\n",
    "            text_prompt,\n",
    "            box_threshold=0.25\n",
    "        )\n",
    "        \n",
    "        segmentor.visualize_results(image, boxes, labels, masks, text_prompt)\n",
    "\n",
    "def interactive_video_segmentation():\n",
    "    \"\"\"Interactive function for video segmentation\"\"\"\n",
    "    print(\"=== Interactive Video Segmentation ===\")\n",
    "    \n",
    "    # Option to upload or use sample\n",
    "    use_sample = input(\"Use sample video? (y/n): \").lower() == 'y'\n",
    "    \n",
    "    if use_sample:\n",
    "        video_path = download_sample_video()\n",
    "    else:\n",
    "        print(\"Upload your video:\")\n",
    "        uploaded = files.upload()\n",
    "        video_path = list(uploaded.keys())[0]\n",
    "    \n",
    "    text_prompt = input(\"Enter text prompt for video segmentation: \")\n",
    "    max_frames = int(input(\"Max frames to process (default 20): \") or \"20\")\n",
    "    \n",
    "    print(f\"\\nProcessing video...\")\n",
    "    frames, segments, labels = video_seg.segment_video(\n",
    "        video_path,\n",
    "        text_prompt,\n",
    "        max_frames=max_frames\n",
    "    )\n",
    "    \n",
    "    if frames is not None:\n",
    "        video_seg.visualize_video_results(\n",
    "            frames, segments, labels, text_prompt,\n",
    "            sample_frames=min(5, len(frames))\n",
    "        )\n",
    "        \n",
    "        save_output = input(\"Save output video? (y/n): \").lower() == 'y'\n",
    "        if save_output:\n",
    "            video_seg.save_video_with_masks(\n",
    "                frames, segments,\n",
    "                output_path='interactive_output.mp4'\n",
    "            )\n",
    "\n",
    "# Uncomment to run interactive demos:\n",
    "# interactive_image_segmentation()\n",
    "# interactive_video_segmentation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb24278",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 10: Quick Test Suite\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"🎉 Setup Complete! SAM 2 Text-Prompted Segmentation Ready\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nQuick Start Options:\")\n",
    "print(\"1. Run Cell 5 for automatic image segmentation demo\")\n",
    "print(\"2. Run Cell 8 for automatic video segmentation demo\")\n",
    "print(\"3. Uncomment functions in Cell 9 for interactive mode\")\n",
    "print(\"\\nExample usage:\")\n",
    "print(\"  image, boxes, labels, masks = segmentor.segment_image('image.jpg', 'cat')\")\n",
    "print(\"  frames, segments, labels = video_seg.segment_video('video.mp4', 'person')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce27a1b5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
